{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9ae855d-df6d-4e6e-aab0-d2d9c1318216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.11/site-packages (2.2.2)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.17.2-cp311-cp311-macosx_10_13_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.2.2-cp311-cp311-macosx_10_13_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torchvision-0.17.2-cp311-cp311-macosx_10_13_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.2.2-cp311-cp311-macosx_10_13_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchvision, torchaudio\n",
      "Successfully installed torchaudio-2.2.2 torchvision-0.17.2\n"
     ]
    }
   ],
   "source": [
    "#java 없으면 설치 필요 링크 참고!!\n",
    "#https://chan-lab.tistory.com/15\n",
    "\n",
    "#torch도 다운 필요시\n",
    "!pip install torch torchvision torchaudio\n",
    "#ㄴ 작성 후 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80eeb3e2-f8c9-4458-9dbb-77317b6df167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from konlpy.tag import Okt\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f7bc85-3e4e-4ec2-8968-73621663b04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess:\n",
    "    okt=Okt() #한글 토큰화를 위한 객체\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.file_path=''\n",
    "        self.file_name=''\n",
    "        self.data=[]\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        text = text.lower()  # 소문자 변환\n",
    "        text = re.sub(r\"[^가-힣a-zA-Z0-9\\s]\", \"\", text)  # 특수 문자 제거\n",
    "        text = re.sub(r\"\\s+\", \" \", text)  # 여러 공백을 하나의 공백으로\n",
    "        return text.strip()\n",
    "\n",
    "    def tokenize_korean(self,text): #한글 토큰화 함수\n",
    "        return self.okt.morphs(text)\n",
    "\n",
    "    def tokenize_english(self,text): # 영어 토큰화 함수\n",
    "        return word_tokenize(text)\n",
    "        \n",
    "    def clean_columns(self):\n",
    "        file_data=self.file_path+'/'+self.file_name+'.xlsx' #데이터셋 로드\n",
    "        data = pd.read_excel(file_data)\n",
    "        \n",
    "        self.data = data[['원문', '번역문']].copy() #읽어들인 파일을 복제해서 작업(원본을 유지하기위함)\n",
    "        self.data.dropna(subset=['원문', '번역문'], inplace=True)  #공백지우기\n",
    "\n",
    "        self.data['원문'] = self.data['원문'].apply(self.clean_text) #한,영 문장에 clean_text함수를 적용한 값을 저장\n",
    "        self.data['번역문'] = self.data['번역문'].apply(self.clean_text)\n",
    "\n",
    "        self.data['원문_토큰'] = self.data['원문'].apply(self.tokenize_korean) #한,영 문장 토큰화\n",
    "        self.data['번역문_토큰'] = self.data['번역문'].apply(self.tokenize_english)\n",
    " \n",
    "    def devide_files(self): #데이터셋을 train, test, validation set으로 나눔\n",
    "        train_data, test_data = train_test_split(self.data, test_size=0.2, random_state=42)\n",
    "        val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "        #train set=80%, test set=10%, validation set=10%\n",
    "        \n",
    "        train_data.to_csv(self.file_path+'/'+self.file_name+'_train.csv', index=False) #나눈 데이터셋들을 다른이름으로 저장\n",
    "        val_data.to_csv(self.file_path+'/'+self.file_name+'_val.csv', index=False)\n",
    "        test_data.to_csv(self.file_path+'/'+self.file_name+'_test.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b443f4a8-2762-4739-9716-19ccf1ee312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 오류날 시 에러 멘트 보고 실행\n",
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cddd84-74e7-4689-94b8-b0d276913e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 오류날 시 에러멘트 보고 실행\n",
    "import nltk\n",
    "nltk.download('punkt_tab') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23243060-2900-4a95-96fd-5df86160b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 수행\n",
    "a=preprocess()\n",
    "a.file_path='.'\n",
    "a.file_name='대화체'\n",
    "a.clean_columns()\n",
    "a.devide_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c546e37c-a4d9-453d-8a46-693f8d127723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp311-cp311-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-17.0.0-cp311-cp311-macosx_10_15_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (2.1.4)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading datasets-3.0.0-py3-none-any.whl (474 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.25.0-py3-none-any.whl (436 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.4/436.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp311-cp311-macosx_10_15_x86_64.whl (29.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading safetensors-0.4.5-cp311-cp311-macosx_10_12_x86_64.whl (392 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m392.3/392.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-macosx_10_12_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-macosx_10_9_x86_64.whl (31 kB)\n",
      "Installing collected packages: xxhash, tqdm, safetensors, requests, pyarrow, dill, torch, multiprocess, huggingface-hub, tokenizers, transformers, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.2\n",
      "    Uninstalling pyarrow-14.0.2:\n",
      "      Successfully uninstalled pyarrow-14.0.2\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.75 requires requests_mock, which is not installed.\n",
      "conda-repo-cli 1.0.75 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.0.0 dill-0.3.8 huggingface-hub-0.25.0 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 safetensors-0.4.5 tokenizers-0.19.1 torch-2.2.2 tqdm-4.66.5 transformers-4.44.2 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "#트랜스포머 다운\n",
    "!pip install torch transformers datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f36f66b2-6332-4910-abf6-071d4330afa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Sample:\n",
      "                  원문                                    번역문  \\\n",
      "0  한정식 세트를 주문하고 싶습니다  id like to order the korean table set   \n",
      "\n",
      "                                    원문_토큰  \\\n",
      "0  ['한정식', '세트', '를', '주문', '하고', '싶습니다']   \n",
      "\n",
      "                                              번역문_토큰  \n",
      "0  ['id', 'like', 'to', 'order', 'the', 'korean',...  \n",
      "\n",
      "Validation Data Sample:\n",
      "                         원문                                         번역문  \\\n",
      "0  모드코s와 무코신일 약을 같이 먹어도 되나요  can i take modeucos and mucosinil together   \n",
      "\n",
      "                                               원문_토큰  \\\n",
      "0  ['모드', '코', 's', '와', '무코', '신일', '약', '을', '같...   \n",
      "\n",
      "                                              번역문_토큰  \n",
      "0  ['can', 'i', 'take', 'modeucos', 'and', 'mucos...  \n",
      "\n",
      "Test Data Sample:\n",
      "                        원문                                                번역문  \\\n",
      "0  저는 약간 맵기는 한데 맛있게 매운맛인데요  its a bit spicy but isnt it spicy in a delicio...   \n",
      "\n",
      "                                               원문_토큰  \\\n",
      "0  ['저', '는', '약간', '맵', '기는', '한데', '맛있게', '매운',...   \n",
      "\n",
      "                                              번역문_토큰  \n",
      "0  ['its', 'a', 'bit', 'spicy', 'but', 'isnt', 'i...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#csv 파일에서 데이터셋 로드\n",
    "train_data = pd.read_csv('./대화체_train.csv')\n",
    "val_data = pd.read_csv('./대화체_val.csv')\n",
    "test_data = pd.read_csv('./대화체_test.csv')\n",
    "\n",
    "#샘플 데이터 출력\n",
    "print(\"Train Data Sample:\")\n",
    "print(train_data.head(1))  #train 데이터셋의 첫 번째 행을 출력\n",
    "\n",
    "print(\"\\nValidation Data Sample:\")\n",
    "print(val_data.head(1))  #validation 데이터셋의 첫 번째 행을 출력\n",
    "\n",
    "print(\"\\nTest Data Sample:\")\n",
    "print(test_data.head(1))  #test 데이터셋의 첫 번째 행을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a42910d2-81ab-412d-b75d-e93188c6555f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Hugging Face에서 토크나이저 load\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-ko-en\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m MarianTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#한 번 토큰화한 데이터를 HuggingFace의 입력 형식으로 변환\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_data_for_huggingface\u001b[39m(dataset):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1543\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1543\u001b[0m requires_backends(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_backends)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1531\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1529\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer\n",
    "\n",
    "#Hugging Face에서 토크나이저 load\n",
    "model_name = \"Helsinki-NLP/opus-mt-ko-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#한 번 토큰화한 데이터를 HuggingFace의 입력 형식으로 변환\n",
    "def prepare_data_for_huggingface(dataset):\n",
    "    inputs = dataset['원문_토큰'].apply(lambda x: \" \".join(eval(x))).tolist()  # 토큰 리스트를 문자열로 변환\n",
    "    labels = dataset['번역문_토큰'].apply(lambda x: \" \".join(eval(x))).tolist()\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "#Train, Validation, Test 데이터를 위 함수를 이용해 준비\n",
    "train_inputs, train_labels = prepare_data_for_huggingface(train_data)\n",
    "val_inputs, val_labels = prepare_data_for_huggingface(val_data)\n",
    "test_inputs, test_labels = prepare_data_for_huggingface(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d195104c-060e-4b14-a2b2-f51b90be6f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "#오류 메세지 뜰 떄 필요시 수\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67ba6f4-4222-435b-8e7f-3e13a424c4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 encoding\n",
    "\n",
    "def encode_data(inputs, labels, tokenizer):\n",
    "    encodings = tokenizer(inputs, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    label_encodings = tokenizer(labels, truncation=True, padding=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    #패딩 토큰을 -100으로 하여 손실 계산 시 무시되도록 함\n",
    "    def replace_pad_token_with_ignore_index(labels, pad_token_id):\n",
    "        return [[(label if label != pad_token_id else -100) for label in labels_example] for labels_example in labels]\n",
    "\n",
    "    labels = replace_pad_token_with_ignore_index(label_encodings, tokenizer.pad_token_id)\n",
    "\n",
    "    return encodings, labels\n",
    "\n",
    "#데이터 encoding\n",
    "train_encodings, train_labels = encode_data(train_inputs, train_labels, tokenizer)\n",
    "val_encodings, val_labels = encode_data(val_inputs, val_labels, tokenizer)\n",
    "test_encodings, test_labels = encode_data(test_inputs, test_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a9448-d53c-4e7b-b374-2c7dd8a347e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch로 데이터셋으로 변환한 데이터 생성\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 데이터셋 생성\n",
    "train_dataset = TranslationDataset(train_encodings, train_labels)\n",
    "val_dataset = TranslationDataset(val_encodings, val_labels)\n",
    "test_dataset = TranslationDataset(test_encodings, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1167049-efe8-4c42-8f15-adc6735b1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#오류 메세지 확인 후 필요시 수행\n",
    "pip install wrapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f7ca6a-b767-49e3-a52f-d2d8311f58b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#오류 메세지 확인 후 필요시 수행\n",
    "pip install --upgrade transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be55f838-396e-45e0-b281-df97557e922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "# 모델 로드\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 데이터 collator 정의\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# 학습 파라미터 설정\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\", #모델체크포인트와 결과가 저장될 dir\n",
    "    eval_strategy=\"steps\",#epoch대신 더 자주 평가되도록\n",
    "    eval_steps=500,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,#저장할 체크포인트 최대\n",
    "    num_train_epochs=3,\n",
    "    gradient_accumulation_steps=2,\n",
    "    predict_with_generate=True,\n",
    ")\n",
    "\n",
    "# Trainer 설정 및 학습\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363c3acc-d688-4493-9960-b162249c3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 학습\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630945aa-007c-466f-8957-7a4d293c5ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 평가\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0021036e-5a53-4214-8926-6be92f2179fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 데이터로 예측 수행\n",
    "predictions = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c1d67-12d1-42e0-ba52-e43261bd0f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#주어진 한글문장을 영어로 번역하는 함수\n",
    "def translate_text(example_text, model_name=\"Helsinki-NLP/opus-mt-ko-en\"):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - example_text (str): 번역할 한글 문장\n",
    "    - model_name (str): 사용할 모델의 이름\n",
    "    \n",
    "    Returns:\n",
    "    - str: 번역된 영어 문장\n",
    "    \"\"\"\n",
    "    #모델과 토크나이저 로드\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "    #예시 문장 토큰화\n",
    "    example_input = tokenizer(example_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    #예측 수행\n",
    "    model.eval()  # 모델을 평가 모드로 전환\n",
    "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "        outputs = model.generate(**example_input)\n",
    "\n",
    "    #토큰 문자열로 변환\n",
    "    predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return predicted_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a95cd47-b128-4499-8c6f-23cd5c5c04e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#함수 사용 예시\n",
    "example_text = \"여기에 번역하고 싶은 한글 문장을 입력하세요.\"\n",
    "translated_text = translate_text(example_text)\n",
    "print(\"예시 문장:\", example_text)\n",
    "print(\"번역 결과:\", translated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
